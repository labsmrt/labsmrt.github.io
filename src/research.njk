---
layout: "page"
title: "Research"
description: "See what's happening."
permalink: "/research/"
eleventyNavigation:
    key: "Research"
    order: 0
style: {
    blogIndex: "#blog-grid { --cols-size: 400px }",
}
---

{% renderTemplate "md" %}

Segmentation and Rhythms in Speech and Music
---
Speech and music are auditory sequences composed of basic building blocks (e.g. notes or syllables) organized into hierarchical structures over multiple timescales (e.g. words and sentences, or beats and musical phrases) — and they unfold linearly in time. Extracting perceptually the building blocks from physical signals does not come for free: acoustic waveforms carrying speech and music are continuous, without clear boundaries marking syllables or chords (imagine, for example, if there were no spacing between words in written text). How does the brain segment the continuous speech and music streams into event segments? In our recent studies, we found that the brain relies on rhythmic structures and prior knowledge of temporal structures to segment continuous auditory streams into perceptual units for further processing.

Prior Knowledge and Prediction
---
We always know some knowledge of temporal regularities in the world: when the sun will rise, when someone is going to finish her or his sentences, and when a melody starts and ends.  How does the brain learn the temporal regularities and use such prior knowledge to predict the future? In a recent study, we found that listeners can learn statistical distributions of events in time and use such knowledge to predict future time points, which is underlied by neural power modulation and a phenomenon of phase precession.

Where are Rhythms in the Brain and Artificial Systems
---
Early sensory experience shapes how we perceive speech; Infants show sensitivity to their native speech rhythms very early. One explanation for infants developing native-language-specific tuning is that they have learned the statistical distribution of rhythm variations in their native language; the native speakers’ brain is crystalized for the statistical distribution of their native speech rhythms. Our question is: where are the rhythms stored and how does the brain use the prior knowledge of rhythms to make predictions? To answer this question, we conduct neuroimaging recording (e.g., EEG/MEG/fMRI) on people learning new L2 rhythms and temporal statistical distributions and, correspondingly, train neural network models to learn rhythms and temporal regularities. By probing the brain and dissecting the artificial system, we hope to reveal where and how the rhythms are stored.

{% endrenderTemplate %}
