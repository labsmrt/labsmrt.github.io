<!doctype html><html dir=ltr lang=en><head><meta charset=utf-8><meta content="width=device-width,initial-scale=1" name=viewport><title>Research - SMRT Lab</title><meta content="See what&amp;#39;s happening." name=description><link href="/css/main.css?=v1.0.0" rel=preload as=style><meta content=summary_large_image name=twitter:card><meta content="Research - SMRT Lab" name=twitter:title><meta content="See what&amp;#39;s happening." name=twitter:description><meta content=https://labsmrt.github.io//images/preview.jpg name=twitter:image><meta content="Research - SMRT Lab" name=og:title><meta content="See what&amp;#39;s happening." name=og:description><meta content=website property=og:type><meta content=https://labsmrt.github.io//research/ property=og:url><meta content=https://labsmrt.github.io//images/preview.jpg property=og:image><meta content=image/png property=og:image:type><meta content=1200 property=og:image:width><meta content=630 property=og:image:height><link href=https://labsmrt.github.io//research/ rel=canonical><link href=https://labsmrt.github.io//feed.xml rel=alternate title="" type=application/rss+xml><meta content="Eleventy v1.0.1" name=generator><meta content=#6001ff name=theme-color><meta content="SMRT Lab" name=apple-mobile-web-app-title><meta content="SMRT Lab" name=application-name><meta content=#6001ff name=msapplication-TileColor><link href=/favicon.ico rel=icon><link href="/css/main.css?=v1.0.0" rel=stylesheet><style>#blog-grid{--cols-size:400px}</style><script defer src="/js/main.js?=v1.0.0"></script></head><body class="flex flex-col min-h-screen"><a href=#page-main class=skip-link>Go to main content</a><header class="page-header sticky top-0"><div class="flex items-center justify-between container md-py-4 py-2"><a href=/ class="flex-shrink logo"><img alt="SMRT Lab" src=/images/logo.jpg height=50 width=50></a><div class="flex items-center"><nav class="hidden sm-block"><ul class="flex-wrap -row menu"><li><a href=/research/ class=is-active aria-current=page>Research</a></li><li><a href=/methodologies/ >Methodologies</a></li><li><a href=/team/ >Team</a></li><li><a href=/collaboration/ >Collaboration</a></li><li><a href=/publications/ >Publications</a></li><li><a href=/life/ >Life</a></li><li><a href=/Participant/ >Participant</a></li></ul></nav><button class="-icon btn ms-4 sm-hidden" type=button id=show-offcanvas-menu><svg fill=currentColor height=24 width=24><title>Open menu</title><use href=/images/sprite.svg#menu /></svg></button></div></div></header><dialog class="offcanvas-end px-2 py-0" id=offcanvas-menu><div class="flex items-center justify-between px-3 py-3"><a href=/ class="flex-shrink logo"><img alt="SMRT Lab" src=/images/logo.jpg height=50 width=50> </a><button class="-icon btn close" type=button><svg fill=currentColor height=24 width=24><title>Close menu</title><use href=/images/sprite.svg#close /></svg></button></div><div class="dialog-inner flex-auto"><nav class=menu aria-label="Offcanvas menu"><li><a href=/research/ class=is-active aria-current=page>Research</a></li><li><a href=/methodologies/ >Methodologies</a></li><li><a href=/team/ >Team</a></li><li><a href=/collaboration/ >Collaboration</a></li><li><a href=/publications/ >Publications</a></li><li><a href=/life/ >Life</a></li><li><a href=/Participant/ >Participant</a></li></nav></div></dialog><main class="flex-grow page-container" id=page-main><div class=container></div><div style="background-color:var(--color-theme); color:white; text-align:center;"><p class=h1 style="padding-top: 30px;">Research</p><p style="padding-bottom: 35px;">See what&#39;s happening.</p></div><article class="container my-6"><h2>Rhythm in Speech and Music</h2><p><img alt=a1 src=/images/a1.jpg style="float: right; margin-left: 10px;"><font size=6>R</font>hythms organize time, akin to how patterns and contours structure space. In both speech and music, rhythmic elements help listeners segment content. Our research explores how the brain utilizes these temporal cues to decode continuous auditory information into distinct units. In speech, we focus on transforming complex sound sequences into semantically clear units, emphasizing sentence-level prosody (Teng et al., 2020). In music, we study how basic elements like notes and beats form higher-level structures, like musical phrases (Teng, Larrouy-Maestri, &amp; Poeppel, 2021). Our work has implications for diagnosing and treating speech and language disorders, cognitive rehabilitation in conditions like Parkinson's, and developing rhythm-based therapies to improve motor coordination and cognitive function.</p><blockquote><p>Teng, X., Larrouy-Maestri, P., &amp; Poeppel, D. (2021). Segmenting and Predicting Musical Phrase Structure Exploits Neural Gain Modulation and Phase Precession. BioRxiv, 2021-07.</p><p>Chang, A., Teng, X., Assaneo, F., &amp; Poeppel, D. (2022). Amplitude modulation perceptually distinguishes music and speech. PsyArXiv</p><p>Teng, X., Ma, M., Yang, J., Blohm, S., Cai, Q., &amp; Tian, X. (2020). Constrained structure of ancient Chinese poetry facilitates speech content grouping. Current Biology, 30(7), 1299-1305.</p></blockquote><br><br><br><br><br><h2>Beyond and Within Rhythm: Active Sensing and Embodied Intelligence</h2><br><br><br><img alt=a2 src=/images/a2.jpg style="float: left; margin-left: 1px;"><font size=6>I</font>n addition to exploring the role of rhythms in speech and music, our research aims to identify broader cognitive and neural principles governing rhythm processing. We posit that both the brain and body leverage external (e.g., poetic meter) and internal (e.g., neural oscillations, heartbeat) rhythms for proactive sensory engagement and internal communication. This aligns with the concept of 'active sensing,' where organisms intentionally interact with their environment to efficiently sample sensory data. Our focus is on how external and internal rhythms help the brain anticipate and process incoming information. These insights allow us to derive computational principles, extending the applicability of our research to sensory deficit diagnosis and robotics.<br><br><br><br><br><br><br><br><br><br><br><h2>Rhythm in Tradition and Culture</h2><br><img alt=a3 src=/images/a3.jpg style="float: right; margin-left: 10px;"><font size=6>W</font>e aim for our research to enrich local society by deepening our understanding of ourselves, our culture, and our heritage. Traditional Chinese art forms like Kun Opera employ coherent auditory and visual rhythms for emotional and aesthetic expression. Unlike classic Western art, which has been extensively investigated, traditional Chinese art remains largely unexplored. Leveraging the methodologies and theoretical frameworks developed in our research, we delve into these artistic rhythms. Specifically, we study the interaction of auditory and visual rhythms in traditional art forms, identify distinct quantitative traits with signal processing and AI, and assess how audiences engage emotionally and aesthetically with these rhythms.<blockquote><p>Teng, X., Ma, M., Yang, J., Blohm, S., Cai, Q., &amp; Tian, X. (2020). Constrained structure of ancient Chinese poetry facilitates speech content grouping. Current Biology, 30(7), 1299-1305.</p></blockquote><br><br><br><br><br><br><br><br><br><h2>Rhythm in Artificial Neural Network and Tools</h2><br><br><img alt=a4 src=/images/a4.jpg style="float: left; margin-left: 1px;"><font size=6>W</font>e employ recurrent neural networks (RNNs) to simulate and analyze rhythms, aiming to elucidate underlying mechanisms and develop analytical tools. Computational modeling enables us to examine the functional goals of observed rhythms, while perturbing rhythms in artificial systems generates predictions for real-world scenarios. Additionally, analyzing artificial rhythms contributes to the development of tools for experimental data assessment. Specifically, we train RNNs on behavioral tasks that parallel our experiments. We then dissect the trained networks to comprehend how rhythms are generated and their functional roles (Teng and Zhang, 2021). Furthermore, we use RNN-driven agents to interact with simulated environments, exploring how rhythms facilitate interactions between the brain, body, and environment.<blockquote><p>Teng, X., &amp; Zhang, R. Y. (2021). Sequential Temporal Anticipation Characterized by Neural Power Modulation and in Recurrent Neural Networks. bioRxiv, 2021-10.<br><br><br>Chen, X., Teng, X., Chen, H., Pan, Y., &amp; Geyer, P. (2024). Toward reliable signals decoding for electroencephalogram: A benchmark study to EEGNeX. Biomedical Signal Processing and Control, 87, 105475.</p></blockquote></article></main><footer class="mt-8 page-footer"><div class="container auto-grid"><nav><p class=h5>Contact</p>Department of Psychology<br>The Chinese University of Hong Kong<br>Shatin, N.T.<br>Hong Kong SAR<p></p><div class="flex flex-wrap mx-n2 socials"><a href=mailto:labteng@gmail.com class=social-icon target=_blank title=mail><svg fill=currentColor height=24 width=24><title>mail</title><use href=/images/socials.svg#mail></use></svg> &nbsp </a><a href=https://github.com/labsmrt class=social-icon target=_blank title=github><svg fill=currentColor height=24 width=24><title>github</title><use href=/images/socials.svg#github></use></svg> &nbsp</a></div><p></p></nav><div></div><div></div><div class="h1 weight-bold" style=display:flex;align-items:center;justify-content:center;>SMRT Lab</div><small class=copyright>Â©2022 SMRT Lab. All Rights Reserved.</small></div></footer></body></html>