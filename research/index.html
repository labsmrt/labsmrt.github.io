<!doctype html><html dir=ltr lang=en><head><meta charset=utf-8><meta content="width=device-width,initial-scale=1" name=viewport><title>Research - SMRT Lab</title><meta content="See what&amp;#39;s happening." name=description><link href="/css/main.css?=v1.0.0" rel=preload as=style><meta content=summary_large_image name=twitter:card><meta content="Research - SMRT Lab" name=twitter:title><meta content="See what&amp;#39;s happening." name=twitter:description><meta content=https://labsmrt.github.io//images/preview.jpg name=twitter:image><meta content="Research - SMRT Lab" name=og:title><meta content="See what&amp;#39;s happening." name=og:description><meta content=website property=og:type><meta content=https://labsmrt.github.io//research/ property=og:url><meta content=https://labsmrt.github.io//images/preview.jpg property=og:image><meta content=image/png property=og:image:type><meta content=1200 property=og:image:width><meta content=630 property=og:image:height><link href=https://labsmrt.github.io//research/ rel=canonical><link href=https://labsmrt.github.io//feed.xml rel=alternate title="" type=application/rss+xml><meta content="Eleventy v1.0.1" name=generator><meta content=#6001ff name=theme-color><meta content="SMRT Lab" name=apple-mobile-web-app-title><meta content="SMRT Lab" name=application-name><meta content=#6001ff name=msapplication-TileColor><link href=/favicon.ico rel=icon><link href="/css/main.css?=v1.0.0" rel=stylesheet><style>#blog-grid{--cols-size:400px}</style><script defer src="/js/main.js?=v1.0.0"></script></head><body class="flex flex-col min-h-screen"><a href=#page-main class=skip-link>Go to main content</a><header class="page-header sticky top-0"><div class="flex items-center justify-between container md-py-4 py-2"><a href=/ class="flex-shrink logo"><img alt="SMRT Lab" src=/images/logo.jpg height=50 width=50></a><div class="flex items-center"><nav class="hidden sm-block"><ul class="flex-wrap -row menu"><li><a href=/research/ class=is-active aria-current=page>Research</a></li><li><a href=/methodologies/ >Methodologies</a></li><li><a href=/team/ >Team</a></li><li><a href=/collaboration/ >Collaboration</a></li><li><a href=/publications/ >Publications</a></li><li><a href=/life/ >Life</a></li><li><a href=/Participant/ >Participant</a></li></ul></nav><button class="-icon btn ms-4 sm-hidden" type=button id=show-offcanvas-menu><svg fill=currentColor height=24 width=24><title>Open menu</title><use href=/images/sprite.svg#menu /></svg></button></div></div></header><dialog class="offcanvas-end px-2 py-0" id=offcanvas-menu><div class="flex items-center justify-between px-3 py-3"><a href=/ class="flex-shrink logo"><img alt="SMRT Lab" src=/images/logo.jpg height=50 width=50> </a><button class="-icon btn close" type=button><svg fill=currentColor height=24 width=24><title>Close menu</title><use href=/images/sprite.svg#close /></svg></button></div><div class="dialog-inner flex-auto"><nav class=menu aria-label="Offcanvas menu"><li><a href=/research/ class=is-active aria-current=page>Research</a></li><li><a href=/methodologies/ >Methodologies</a></li><li><a href=/team/ >Team</a></li><li><a href=/collaboration/ >Collaboration</a></li><li><a href=/publications/ >Publications</a></li><li><a href=/life/ >Life</a></li><li><a href=/Participant/ >Participant</a></li></nav></div></dialog><main class="flex-grow page-container" id=page-main><div class=container></div><div style="background-color:var(--color-theme); color:white; text-align:center;"><p class=h1 style="padding-top: 30px;">Research</p><p style="padding-bottom: 35px;">See what&#39;s happening.</p></div><article class="container my-6"><h2>Rhythm in Speech and Music</h2><p><img alt=a1 src=/images/a1.jpg style="float: right; margin-left: 10px;"><font size=6>R</font>hythms organize our perception of time in ways similar to how patterns and structures help us navigate space. Both speech and music contain rhythmic patterns that assist listeners in identifying meaningful segments within continuous streams of sound. Our lab investigates how the brain, particularly through neural circuits in the dorsal auditory-motor pathway, utilizes rhythmic and temporal cues to interpret complex auditory inputs effectively. In speech, we explore how rhythmic features, such as sentence-level intonation and stress patterns, help listeners clearly understand spoken language (Teng et al., 2020). In music, we examine how fundamental elements like notes and beats combine to form cohesive structures, such as musical phrases (Teng, Larrouy-Maestri, &amp; Poeppel, 2021). Our research integrates the study of rhythm perception with reward processing, reinforcement learning, memory, and temporal mapping, with a particular emphasis on dorsal stream functions. This integrated approach contributes to understanding and developing rhythm-based therapies aimed at addressing speech and language disorders, supporting cognitive rehabilitation in conditions like Parkinson's disease, and enhancing motor coordination and cognitive functions through rhythm-focused interventions.</p><blockquote><p>Teng, X., Larrouy-Maestri, P., &amp; Poeppel, D. (2021). Segmenting and Predicting Musical Phrase Structure Exploits Neural Gain Modulation and Phase Precession. BioRxiv, 2021-07.</p><p>Chang, A., Teng, X., Assaneo, F., &amp; Poeppel, D. (2022). Amplitude modulation perceptually distinguishes music and speech. PsyArXiv</p><p>Teng, X., Ma, M., Yang, J., Blohm, S., Cai, Q., &amp; Tian, X. (2020). Constrained structure of ancient Chinese poetry facilitates speech content grouping. Current Biology, 30(7), 1299-1305.</p></blockquote><br><br><br><br><br><h2>Rhythm, Reward, and Time: How Temporal Patterns Shape Learning, Memory, and Emotion</h2><br><br><br><img alt=a2 src=/images/a2.jpg style="float: left; margin-left: 1px;"><font size=6>O</font>ur research explores how reward, motivation, and reinforcement learning influence temporal processing and perception. We investigate the relationship between temporal patterns, whether external (such as rhythms in music and speech) or internal (like neural oscillations), and the motivational states they induce. Central to our studies is understanding how temporal cues enhance learning, emotional engagement, and the anticipation of rewarding experiences through reinforcement learning mechanisms. This research also examines how pleasurable or rewarding experiences can alter our perception of time, offering scientific insight into common experiences described by sayings like “Pleasant times pass quickly.” Insights from this work aim to contribute to new therapeutic strategies addressing emotional and motivational challenges, cognitive rehabilitation approaches, and technologies designed to leverage temporal-based reinforcement learning interactions.<br><br><br><br><br><br><br><br><br><br><h2>Rhythm in Tradition and Culture</h2><br><img alt=a3 src=/images/a3.jpg style="float: right; margin-left: 10px;"><font size=6>O</font>ur research seeks to enrich local society by exploring how rhythm and reward interact within traditional art forms, deepening our understanding of cultural heritage and artistic expression. We focus on traditional Chinese arts, such as Kun Opera and ancient poetry, which intricately blend speech, musical, and visual rhythms to evoke rewarding emotional and aesthetic experiences. By employing advanced methodologies including signal processing and artificial intelligence, we analyze these rhythmic patterns to uncover their unique quantitative features. Furthermore, we investigate how audiences perceive and emotionally engage with these rhythms, exploring the pleasurable and rewarding experiences provided by traditional cultural arts. This approach helps illuminate how rhythm and reward intertwine in shaping artistic appreciation and cultural identity.<blockquote><p>Teng, X., Ma, M., Yang, J., Blohm, S., Cai, Q., &amp; Tian, X. (2020). Constrained structure of ancient Chinese poetry facilitates speech content grouping. Current Biology, 30(7), 1299-1305.</p></blockquote><br><br><br><br><br><br><br><h2>Rhythm in Artificial Neural Network and Tools</h2><br><br><img alt=a4 src=/images/a4.jpg style="float: left; margin-left: 1px;"><font size=6>O</font>ur research leverages computational methods including reinforcement learning (RL), recurrent neural networks (RNNs), and advanced analytical tools to simulate and analyze rhythmic patterns. Reinforcement learning, in particular, allows us to model how agents learn rhythmic behaviors through reward-based interactions with their environments. Concurrently, we utilize RNNs trained on behavioral tasks analogous to our experimental paradigms, enabling detailed exploration of how rhythms are generated and their functional roles in cognitive processes (Teng and Zhang, 2021). These approaches collectively provide insights into the complex dynamics of rhythm perception and production, facilitating the development of predictive tools for real-world experimental conditions and enhancing our understanding of how rhythmic behaviors optimize interactions between the brain, body, and environment.<blockquote><p>Teng, X., &amp; Zhang, R. Y. (2021). Sequential Temporal Anticipation Characterized by Neural Power Modulation and in Recurrent Neural Networks. bioRxiv, 2021-10.<br><br><br>Chen, X., Teng, X., Chen, H., Pan, Y., &amp; Geyer, P. (2024). Toward reliable signals decoding for electroencephalogram: A benchmark study to EEGNeX. Biomedical Signal Processing and Control, 87, 105475.</p></blockquote></article></main><footer class="mt-8 page-footer"><div class="container auto-grid"><nav><p class=h5>Contact</p>Department of Psychology<br>The Chinese University of Hong Kong<br>Shatin, N.T.<br>Hong Kong SAR<p></p><div class="flex flex-wrap mx-n2 socials"><a href=mailto:labteng@gmail.com class=social-icon target=_blank title=mail><svg fill=currentColor height=24 width=24><title>mail</title><use href=/images/socials.svg#mail></use></svg> &nbsp </a><a href=https://github.com/labsmrt class=social-icon target=_blank title=github><svg fill=currentColor height=24 width=24><title>github</title><use href=/images/socials.svg#github></use></svg> &nbsp</a></div><p></p></nav><div></div><div></div><div class="h1 weight-bold" style=display:flex;align-items:center;justify-content:center;>SMRT Lab</div><small class=copyright>©2022 SMRT Lab. All Rights Reserved.</small></div></footer></body></html>